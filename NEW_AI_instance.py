import hashlib
import struct

import pandas as pd
import tensorflow as tf
import numpy as np
import pickle
import threading

from virus_total_api import VirusTotal_class
from Parsing_CSV import Open_Csv # CSV편집기
from Make_X_only_EXE import Get_X_from_EXE_bin, MAKING_x_y_for_TRAINNING
class AI_Instance():
    def __init__(self, Server_VT_api:str=None,VT_Csv_Path:str=None,  Main_Csv_Path:str=None, Save_Index:str=None): # Server_VT_api 는 Train할 때만 사용.  나머지는 Client부담임
        try:
            self.LOCK_for_Main_Csv = threading.Lock() # 메인 CSV 접근
            self.LOCK_for_VT_Csv = threading.Lock() # VT결과저장 CSV 접근
            self.LOCK_for_Model = threading.Lock() # 모델 저징/로드
            self.LOCK_for_extra_save = threading.Lock() # x,y 저장 로드
            if ( Server_VT_api == None ) or (Main_Csv_Path == None) or (Save_Index == None) or (VT_Csv_Path == None):
                print("생성자 파라미터중 몇가지가 비어있습니다.")
                return

            self.Server_VT_api = Server_VT_api
            self.VT_Csv_Path = VT_Csv_Path
            self.Main_Csv_Path = Main_Csv_Path
            self.Save_Index = Save_Index
            self.Server_VT_instance = VirusTotal_class(
                API_KEY= self.Server_VT_api
            )

            self.Main_Csv_Manager_instance = Open_Csv( # 메인 CSV 쓰기/읽기 인스턴스
                Input_Csv_Path= self.Main_Csv_Path
            )

            self.VT_Csv_Manager_instance = Open_Csv(  # VT CSV 쓰기/읽기 인스턴스
                Input_Csv_Path=self.VT_Csv_Path
            )

        except:
            print("AI 인스턴스 자체를 생성할 수 없습니다.")
            return

    def Start_Train(self,
                    epoch: int=100, batch_size: int=2, validation_split_num: float=0.2,
                    learning_rate:float=0.001,
                    patience:int=100//4, min_delta:float=0.1): # 병렬 스레드

        EXE_PATH_list = []
        EXE_SHA256_list = []
        EXE_y_list = []
        ''' 잠시 LOCK하여 Main_CSV 추출'''
        with self.LOCK_for_Main_Csv:

            df = pd.read_csv(self.Main_Csv_Path)
            EXE_PATH_list = [f'{i}' for i in df['values']]
            EXE_SHA256_list = [f'{i}' for i in df['sha256']]
            EXE_y_list = [f'{i}' for i in df['types']]
        #print([ data for data in zip(EXE_PATH_list,EXE_SHA256_list)])

        result_X=None
        result_y=None
        with self.LOCK_for_VT_Csv:
            ''' X / y 제작 및 저장. ''' # y 제작은 안에서 진행함.
            result_X, result_y = MAKING_x_y_for_TRAINNING(
                EXE_PATH_LIST_from_Main_csv= zip(EXE_PATH_list,EXE_SHA256_list, EXE_y_list),
                SAVE_INDEX = self.Save_Index,
                SAVE_LOCK=self.LOCK_for_extra_save,
                SERVER_VT_CSV_MANAGER=self.VT_Csv_Manager_instance,
                Server_VT_instance=self.Server_VT_instance
            )

        ''' 속도를 위한 전처리기 '''
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        X_with_scaler = scaler.fit_transform(result_X)

        ''' 훈련 세트 제작. '''
        from sklearn.model_selection import train_test_split
        x_train, x_valid, y_train, y_valid = train_test_split(X_with_scaler, result_y, test_size=0.2)

        from Model_Configure import Make_Model_Layers # 층 자동화 생성 함수
        result_model = Make_Model_Layers(
            X=result_X,
            y=result_y,
            ALL_of_LAYERS_count=6
        )
        # 모델 컴파일
        result_model.compile(
            #optimizer='adam',
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), # Adam을 쓰지만, 훈련 속도를 늦춰 개선
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        # EarlyStopping 콜백 추가
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',  # 검증 손실을 모니터링
            patience=patience,  # 개선되지 않는 에포크 수
            min_delta=min_delta,  # 개선될 최소 변화량
            restore_best_weights=True  # 최고의 가중치를 복원
        )


        # 모델 학습
        result_model.fit(
            x_train, y_train,
            epochs=epoch, batch_size=batch_size, validation_split=validation_split_num,
            callbacks=[early_stopping]
        )


        # 모델 개션 필요! ( val_loss 가 계속 올라간다면 멈춰라 )

        ''' 모델,스케일러 -> SAVE! '''
        with self.LOCK_for_Model:
            try:
                tf.keras.models.save_model(
                    model=result_model,
                    filepath=f'{self.Save_Index}_model'
                ) # 모델 세이브 default
            except:
                try:
                    result_model.export(f'{self.Save_Index}_model')  # 모델 세이브 export는 최신 텐서플로우에서 작동
                except:
                    result_model.save(f'{self.Save_Index}_model')

            try:
                with open(f"{self.Save_Index}_scaler", 'wb') as f:
                    pickle.dump(scaler,f,protocol=5)
            except:
                print("스케일러 저장실패;;")

    def Start_Prediction(self,Client_VT_api:str=None, EXE_bin:bytes=None): # 병렬 스레드
        if Client_VT_api == None : print("Client_VT_api 를 입력하십시오.");return
        ''' 훈련된 모델 obj load'''
        Model = None
        X_for_dll_api_section_malware = None # 훈련시 생성된 동적 데이터세트 추출(dll,api,section,api+malware) 4가지
        y_for_predict = None # 훈련시 생성된 y 칼럼
        Scaler = None # 훈련시 생성된 스케일러
        with self.LOCK_for_Model:
            try:
                Model = tf.keras.models.load_model(
                    filepath=f'{self.Save_Index}_model'
                )
            except:
                print("로드 실패")
                return

            try:
                with open(f"{self.Save_Index}_x", 'rb') as f_x:
                    with open(f"{self.Save_Index}_y", 'rb') as f_y:
                        X_for_dll_api_section_malware = pickle.load(f_x)
                        y_for_predict = pickle.load(f_y)
            except:
                print("x,y 추출 실패")
                return

            try:
                with open(f"{self.Save_Index}_scaler", 'rb') as f_scaler:
                    Scaler = pickle.load(f_scaler)
            except:
                print("scaler 추출 실패")
                return

        print(Model, X_for_dll_api_section_malware, y_for_predict, Scaler)

        ''' 데이터 추출 성공한 이후임 '''


        ''' CLient전용 VT 인스턴스 생성'''
        CLIENT_VT_instance = VirusTotal_class(
            API_KEY=Client_VT_api
        )

        SHA256 = hashlib.sha256(EXE_bin).hexdigest() # 해시구하기 for VT
        print(f"현재 예측중인 EXE의 SHA256 값 -> {SHA256}")
        result_X = None
        is_VT_success = False
        with self.LOCK_for_VT_Csv:
            result_X,is_VT_success = Get_X_from_EXE_bin(
                EXE_bin = EXE_bin,
                loaded_x_for_dll_api_section_Malware=X_for_dll_api_section_malware,
                SHA256 = SHA256,
                Client_VT_instance = CLIENT_VT_instance,
                SERVER_VT_CSV_MANAGER = self.VT_Csv_Manager_instance
            )
        ''' Scaler 적용 '''
        X = Scaler.transform(result_X)# Scalar 전처리

        ''' 예측시작 '''
        complete_Prediction = Model.predict(X)
        print(f"예측결과->>{complete_Prediction}")
        Max_index = np.argmax(complete_Prediction)





        print(f"is_VT_success 결과 -> {is_VT_success}")
        ''' csv 반영 '''
        if is_VT_success == False :
            print("예측할 X를 구할 때, VT가 실패했으므로, Main_CSV에 분석결과를 저장하지 않겠습니다. ")
        else:

            with self.LOCK_for_Main_Csv:
                # SHA256을 구하고 중복 검사 . 중복이 아니면 축적하고 재저장-2
                csv_parsed, X_len, Y_len = self.Main_Csv_Manager_instance.Open_and_Setting()
                if not (any(csv_data[1] == SHA256 for csv_data in csv_parsed)):  # csv_data에 없을 때
                    with open(f".\\{SHA256}.exe", 'wb') as f:  # 현 분석 디렉터리에 저장.
                        f.write(EXE_bin)

                    '''
                        CSV에 등록
                    '''
                    if  self.Main_Csv_Manager_instance.Write_to_Csv( # raw새로 APPEND
                            [f".\\{SHA256}.exe", SHA256, str(y_for_predict[Max_index])]
                    ):
                        print("CSV 성공")
                else:
                    '''
                        CSV에 이미 존재하는 SHA256이 있다면, 그 행의 값을 최신화한다.
                    '''
                    print("최신화시도")
                    if self.Main_Csv_Manager_instance.Rewrite_to_Csv( # 이미 존재하는 row를 수정
                            Input_data=[f".\\{SHA256}.exe", SHA256, str(y_for_predict[Max_index])],
                            is_ALL_change=False,
                            specified_hint=SHA256,
                            index_hint=1,
                            is_Append_column_data=False,

                            Rewrite_column=False,
                            columns_list_for_Rewrite_column=None,
                            is_Append_for_Rewrite_column=False
                    ):
                        print("CSV 성공")


        return (y_for_predict, # y 칼럼 이름(list)
                struct.pack('<f', float(list(complete_Prediction)[0][Max_index])), # 예측후 결정된 칼럼의 1.0~0.0 사이 값 (float->bytes)
                str(y_for_predict[Max_index])) # y 칼럼중 판단된 칼럼 이름 (str)

a = AI_Instance(
    Server_VT_api='b081a247f8bde7d98337dec2a44b9dff7d1eba19bd99a20c04395cea831177ee',
    VT_Csv_Path = 'VT_analysed.csv',
    Main_Csv_Path='SHARE_ai_exe_dataset.csv',
    Save_Index='idk2'
)

a.Start_Train(
    epoch=100,
    batch_size=2,
    patience=(100//4)
)

SAMPLE_EXE = b''
with open("C:\\wget.exe", 'rb') as f:
    SAMPLE_EXE = f.read()

result = a.Start_Prediction(
    EXE_bin=SAMPLE_EXE,
    Client_VT_api = 'b081a247f8bde7d98337dec2a44b9dff7d1eba19bd99a20c04395cea831177ee'
)

print(result)


'''thread_list = []
for i in range(2):
    ace = threading.Thread(target=a.Start_Prediction,args=("b081a247f8bde7d98337dec2a44b9dff7d1eba19bd99a20c04395cea831177ee",SAMPLE_EXE))
    ace.start()

while True:
    pass'''