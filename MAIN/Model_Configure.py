import tensorflow as tf

def Make_one_DENSE_Layer(
        Previous_Model_Layer = None,
        units:int=None,
        activation:str=None,
        regularization_rate:float=0.01
):
    return tf.keras.layers.Dense(
        units=units,
        activation=activation,
        kernel_regularizer=tf.keras.regularizers.l2(regularization_rate) # 정규화 넣으면서 개선됨
    )(Previous_Model_Layer)

def Make_one_DROPOUT_Layer(
        Previous_Model_Layer = None,
        dropout_value:float=0.2
):
    return tf.keras.layers.Dropout(dropout_value)(Previous_Model_Layer)

# 자동으로 입력층 갯수에 따라 자동으로 층을 생성해줌
def Make_Model_Layers(
        X=None,
        y=None,

        ALL_of_LAYERS_count:int = 12,

        epoch: int=100, batch_size: int=2, validation_split_num: float=0.2
):
    X_len = len( X[0] ) #X 하나의 길이를 추출 (입력층 갯수 추출)
    y_len = len(y[0]) # y 하나의 길이를 추출 ( 출력층 갯수 추출 )


    X_model_input = tf.keras.layers.Input(shape=(len(X[0]),)) # 입력층


    ''' 반복하여 층 생성 '''
    X_CURRENT_model = X_model_input # 지속적으로 은닉층을 생성하기 위해 current 변수에 담아서 지속 사용
    available_layer_count = ALL_of_LAYERS_count//2 # 각 층을 for문씩 할 때의 최대 가능 갯수 (파라미터 / 2 의 몫임)
    '''[늘리기]'''
    for Increase_Layer_count in range(available_layer_count+1): # 증가
        if Increase_Layer_count == 0: continue
        if (Increase_Layer_count % 3) == 0:
            X_CURRENT_model = Make_one_DROPOUT_Layer(
                Previous_Model_Layer=X_CURRENT_model
            )
            continue
        else:
            X_CURRENT_model = Make_one_DENSE_Layer(
                Previous_Model_Layer = X_CURRENT_model,
                units= ((X_len/2)//2) * Increase_Layer_count, # 층의 갯수를 더 줄여 개선됨
                activation='relu'
            )

    '''[줄이기]'''
    for decrease_Layer_count in reversed(range(available_layer_count+1)): # 감소
        if decrease_Layer_count == 0 : break
        print(f"decrease_Layer_count % 3 -> {decrease_Layer_count % 3}/{decrease_Layer_count}")
        if (decrease_Layer_count % 3) == 0:
            X_CURRENT_model = Make_one_DROPOUT_Layer(
                Previous_Model_Layer=X_CURRENT_model
            )
            continue
        else:
            X_CURRENT_model = Make_one_DENSE_Layer(
                Previous_Model_Layer = X_CURRENT_model,
                units= ((X_len/2)//2) * decrease_Layer_count,
                activation='relu'
            )
    ''' 마무리에 별도로 DropOut 추가 '''
    X_CURRENT_model = Make_one_DROPOUT_Layer(
        Previous_Model_Layer=X_CURRENT_model
    )
    ''' y 출력층 '''
    output_layer = tf.keras.layers.Dense(y_len, activation='softmax')(X_CURRENT_model) # 최종 출력층
    print(f"After adding Output Layer: {output_layer}")

    # 모델 정의
    model = tf.keras.models.Model(inputs=X_model_input, outputs=output_layer)

    # 모델 요약 출력
    model.summary()
    return model
    # 모델 컴파일
    #model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

    # 모델 학습
    #model.fit(X, y, epochs=epoch, batch_size=batch_size, validation_split=validation_split_num)