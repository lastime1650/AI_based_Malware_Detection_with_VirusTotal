import hashlib
import struct

import pandas as pd
import tensorflow as tf
import numpy as np
import pickle
import threading

from virus_total_api import VirusTotal_class
from Parsing_CSV import Open_Csv # CSV편집기
from Make_X_only_EXE import Get_X_from_EXE_bin, MAKING_x_y_for_TRAINNING
class AI_Instance():
    def __init__(self, Server_VT_api:str=None,VT_Csv_Path:str=None,  Main_Csv_Path:str=None, Save_Index:str=None,
                     
                     Non_Malware_exes_dir:str=".\\Normal_EXE\\", # [v2.1]
                     Non_Malware_run_with_parallel:bool=True # [v2.1]
                    
                    ): # Server_VT_api 는 Train할 때만 사용.  나머지는 Client부담임


        self.LOCK_for_Main_Csv = threading.Lock() # 메인 CSV 접근
        self.LOCK_for_VT_Csv = threading.Lock() # VT결과저장 CSV 접근
        self.LOCK_for_Model = threading.Lock() # 모델 저징/로드
        self.LOCK_for_extra_save = threading.Lock() # x,y 저장 로드
        if ( Server_VT_api == None ) or (Main_Csv_Path == None) or (Save_Index == None) or (VT_Csv_Path == None):
            print("생성자 파라미터중 몇가지가 비어있습니다.")
            return

        self.Server_VT_api = Server_VT_api
        self.VT_Csv_Path = VT_Csv_Path
        self.Main_Csv_Path = Main_Csv_Path
        self.Save_Index = Save_Index
        self.Server_VT_instance = VirusTotal_class(
            API_KEY= self.Server_VT_api
        )

        self.Main_Csv_Manager_instance = Open_Csv( # 메인 CSV 쓰기/읽기 인스턴스
            Input_Csv_Path= self.Main_Csv_Path
        )

        self.VT_Csv_Manager_instance = Open_Csv(  # VT CSV 쓰기/읽기 인스턴스
            Input_Csv_Path=self.VT_Csv_Path
        )

        ''' 
            데이터 세트 만들기 [NEW] 
            MAIN_CSV에 저장해야한다.
        '''

        '''악성 추출'''
        self.parallel_for_Malware_dataset_THREAD:threading.Thread = None # [v2.0]
        self.parallel_for_Malware_dataset_THREAD_is_Terminate = False # [v2.1]
        print("[병렬 스레드 시작] -> self.parallel_for_Malware_dataset_THREAD ") # [v2.0]
        self.parallel_for_Malware_dataset_THREAD = threading.Thread(target=self.parallel_for_Malware_dataset,args=("950d8ef3adefd3bc05a1ed8174877949",)) # [v2.0]
        self.parallel_for_Malware_dataset_THREAD.start() # [v2.0]


        '''일반EXE추출 -> PortableAPP 으로 일반EXE를 이미 DISK에 저장되어 있으면 좋음'''
        # Non_Malware_exes_dir <- 이 경로는 일반 EXE가 들어간 디렉터리를 받는다.-> "악성아님"으로 저장될 것이다.
        self.parallel_for_Normal_dataset_THREAD:threading.Thread = None # [v2.1]
        self.parallel_for_Normal_dataset_THREAD_is_Terminate = False # [v2.1]
        if Non_Malware_exes_dir != None:
            #None이 아닐 때 초기 일반 데이터세트 구현
            print("[병렬 스레드 시작] -> self.parallel_for_Normal_dataset_THREAD ")
            self.parallel_for_Normal_dataset_THREAD =  threading.Thread(target=self.parallel_for_Normal_dataset,
                                                                        args=(Non_Malware_exes_dir,)
                                                                        ) # [v2.1]
            self.parallel_for_Normal_dataset_THREAD.start() # [v2.1]

    # 병렬적으로 일반 데이터 세트를 구축한다.
    def parallel_for_Normal_dataset(self, DIR_:str=None): # [v2.1]

        print("parallel_for_Normal_dataset 실행")
        while True:
            if self.parallel_for_Normal_dataset_THREAD_is_Terminate == True: return
            '''
                PortableAPP을 중심으로 일반 EXE들을 모두 추출한다.
            '''
            tmp_get_Normal_exe_list = []
            for root, dirs, files in os.walk(f"{DIR_}"):
                #print(files)
                if len(files) < 1: print("Normal-> exe가 존재하지 않음");break # 파일 리스트가 존재하지 않을 때는 다시 처음부터!
                for file_name in files:
                    tmp_get_Normal_exe_list.append(f"{DIR_}{file_name}")  # 절대경로

            #print(f"tmp_get_Normal_exe_list --> {tmp_get_Normal_exe_list}")

            for absolute_path in tmp_get_Normal_exe_list:
                #print(f"absolute_path -> {absolute_path}")
                ''' SHA256의 경우, 직접 일일히 구하면서 하는걸로 함. '''
                tmp_bin = b''
                with open(absolute_path, 'rb') as f:
                    tmp_bin = f.read()

                SHA256 = hashlib.sha256(tmp_bin).hexdigest()  # 해시 구함
                #print(f"SHA256:{SHA256} /exename:{absolute_path}")
                with self.LOCK_for_Main_Csv:  # Main.csv에 넣기전에, 중복 검사
                    reuslt_one_row = self.Main_Csv_Manager_instance.Output_one_row(
                        specified_hint=SHA256,
                        index_hint=1
                    )

                    if reuslt_one_row == None:
                        print("reuslt_one_row 가 없음!")
                        # csv 넣기
                        self.Main_Csv_Manager_instance.Write_to_Csv(
                            Input_data=[absolute_path, SHA256, "악성아님"]
                        )

        
    
        
    def parallel_for_Malware_dataset(self, API:str=None): # [v2.0]
        from Malware_Bazaar import Malware_Bazaar_Manager
        '''악성EXE추출'''
        malware_manager = Malware_Bazaar_Manager(
            API=API
        )
        complete_count = 10
        current_count = 0
        while True:
            if complete_count > 1000 :
                print("꽉 찼군요!")
                complete_count = 1000

            RESULT_malware_dataset = malware_manager.Make_sample_to_DISK_for_malware_DATASET(
                file_type="exe",
                get_count=complete_count,
                save_path=".\\Malware_EXE\\"
            )

            if RESULT_malware_dataset == None or len(RESULT_malware_dataset) == 0:
                complete_count += 20
                current_count += 20
                continue
            else:
                for file_name in RESULT_malware_dataset:
                    # ("혹시 Main.csv에 넣기전에, 중복인 경우는 제외해야한다.")

                    # 상호배제
                    with self.LOCK_for_Main_Csv:
                        result_row = self.Main_Csv_Manager_instance.Output_one_row(
                            specified_hint=str(file_name),
                            index_hint=0
                        )
                        if result_row == None:
                            SHA256_from_file_name = (
                            str(list(str(file_name).split("\\"))[len((list(str(file_name).split("\\")))) - 1]).split(".")[
                                0]).split("_from_malware")[0]
                            self.Main_Csv_Manager_instance.Write_to_Csv(
                                Input_data=[file_name, SHA256_from_file_name, "악성"]
                            )

                # 점차 증가형
                complete_count += 20
                current_count += 20

            current_count = len(RESULT_malware_dataset)

            # RESULT_malware_dataset의 길이가 get_count와 다르면, 중간 이상이 있는 것임.
            if complete_count >= current_count:
                print("길이가 다르므로 다시 얻어오겠습니다.")
                # complete_count = complete_count - current_count # 다시 얻어와야할 길이만 구해서 반복
                complete_count += 1
                current_count += 1
                continue
    
    def Start_Train(self,
                    epoch: int=100, batch_size: int=2, validation_split_num: float=0.2,
                    learning_rate:float=0.001,
                    patience:int=100//4, min_delta:float=0.1): # 병렬 스레드

        EXE_PATH_list = []
        EXE_SHA256_list = []
        EXE_y_list = []
        ''' 잠시 LOCK하여 Main_CSV 추출'''
        with self.LOCK_for_Main_Csv:

            df = pd.read_csv(self.Main_Csv_Path)
            EXE_PATH_list = [f'{i}' for i in df['values']]
            EXE_SHA256_list = [f'{i}' for i in df['sha256']]
            EXE_y_list = [f'{i}' for i in df['types']]
        #print([ data for data in zip(EXE_PATH_list,EXE_SHA256_list)])

        result_X=None
        result_y=None
        with self.LOCK_for_VT_Csv:
            ''' X / y 제작 및 저장. ''' # y 제작은 안에서 진행함.
            result_X, result_y = MAKING_x_y_for_TRAINNING(
                EXE_PATH_LIST_from_Main_csv= zip(EXE_PATH_list,EXE_SHA256_list, EXE_y_list),
                SAVE_INDEX = self.Save_Index,
                SAVE_LOCK=self.LOCK_for_extra_save,
                SERVER_VT_CSV_MANAGER=self.VT_Csv_Manager_instance,
                Server_VT_instance=self.Server_VT_instance
            )

        ''' 속도를 위한 전처리기 '''
        from sklearn.preprocessing import MinMaxScaler
        scaler = MinMaxScaler()
        X_with_scaler = scaler.fit_transform(result_X)

        ''' 훈련 세트 제작. '''
        from sklearn.model_selection import train_test_split
        x_train, x_valid, y_train, y_valid = train_test_split(X_with_scaler, result_y, test_size=0.2)

        from Model_Configure import Make_Model_Layers # 층 자동화 생성 함수
        result_model = Make_Model_Layers(
            X=result_X,
            y=result_y,
            ALL_of_LAYERS_count=6
        )
        # 모델 컴파일
        result_model.compile(
            #optimizer='adam',
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate), # Adam을 쓰지만, 훈련 속도를 늦춰 개선
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )

        # EarlyStopping 콜백 추가
        early_stopping = tf.keras.callbacks.EarlyStopping(
            monitor='val_loss',  # 검증 손실을 모니터링
            patience=patience,  # 개선되지 않는 에포크 수
            min_delta=min_delta,  # 개선될 최소 변화량
            restore_best_weights=True  # 최고의 가중치를 복원
        )


        # 모델 학습
        result_model.fit(
            x_train, y_train,
            epochs=epoch, batch_size=batch_size, validation_split=validation_split_num,
            callbacks=[early_stopping]
        )


        # 모델 개션 필요! ( val_loss 가 계속 올라간다면 멈춰라 )

        ''' 모델,스케일러 -> SAVE! '''
        with self.LOCK_for_Model:
            try:
                tf.keras.models.save_model(
                    model=result_model,
                    filepath=f'{self.Save_Index}_model'
                ) # 모델 세이브 default
            except:
                try:
                    result_model.export(f'{self.Save_Index}_model')  # 모델 세이브 export는 최신 텐서플로우에서 작동
                except:
                    result_model.save(f'{self.Save_Index}_model')

            try:
                with open(f"{self.Save_Index}_scaler", 'wb') as f:
                    pickle.dump(scaler,f,protocol=5)
            except:
                print("스케일러 저장실패;;")

    def Start_Prediction(self,Client_VT_api:str=None, EXE_bin:bytes=None): # 병렬 스레드
        if Client_VT_api == None : print("Client_VT_api 를 입력하십시오.");return
        ''' 훈련된 모델 obj load'''
        Model = None
        X_for_dll_api_section_malware = None # 훈련시 생성된 동적 데이터세트 추출(dll,api,section,api+malware) 4가지
        y_for_predict = None # 훈련시 생성된 y 칼럼
        Scaler = None # 훈련시 생성된 스케일러
        with self.LOCK_for_Model:
            try:
                Model = tf.keras.models.load_model(
                    filepath=f'{self.Save_Index}_model'
                )
            except:
                print("로드 실패")
                return

            try:
                with open(f"{self.Save_Index}_x", 'rb') as f_x:
                    with open(f"{self.Save_Index}_y", 'rb') as f_y:
                        X_for_dll_api_section_malware = pickle.load(f_x)
                        y_for_predict = pickle.load(f_y)
            except:
                print("x,y 추출 실패")
                return

            try:
                with open(f"{self.Save_Index}_scaler", 'rb') as f_scaler:
                    Scaler = pickle.load(f_scaler)
            except:
                print("scaler 추출 실패")
                return

        print(Model, X_for_dll_api_section_malware, y_for_predict, Scaler)

        ''' 데이터 추출 성공한 이후임 '''


        ''' CLient전용 VT 인스턴스 생성'''
        CLIENT_VT_instance = VirusTotal_class(
            API_KEY=Client_VT_api
        )

        SHA256 = hashlib.sha256(EXE_bin).hexdigest() # 해시구하기 for VT
        print(f"현재 예측중인 EXE의 SHA256 값 -> {SHA256}")
        result_X = None
        is_VT_success = False
        with self.LOCK_for_VT_Csv:
            result_X,is_VT_success = Get_X_from_EXE_bin(
                EXE_bin = EXE_bin,
                loaded_x_for_dll_api_section_Malware=X_for_dll_api_section_malware,
                SHA256 = SHA256,
                Client_VT_instance = CLIENT_VT_instance,
                SERVER_VT_CSV_MANAGER = self.VT_Csv_Manager_instance
            )
        ''' Scaler 적용 '''
        X = Scaler.transform(result_X)# Scalar 전처리

        ''' 예측시작 '''
        complete_Prediction = Model.predict(X)
        print(f"예측결과->>{complete_Prediction}")
        Max_index = np.argmax(complete_Prediction)





        print(f"is_VT_success 결과 -> {is_VT_success}")
        ''' csv 반영 '''
        if is_VT_success == False :
            print("예측할 X를 구할 때, VT가 실패했으므로, Main_CSV에 분석결과를 저장하지 않겠습니다. ")
        else:

            with self.LOCK_for_Main_Csv:
                # SHA256을 구하고 중복 검사 . 중복이 아니면 축적하고 재저장-2
                csv_parsed, X_len, Y_len = self.Main_Csv_Manager_instance.Open_and_Setting()
                if not (any(csv_data[1] == SHA256 for csv_data in csv_parsed)):  # csv_data에 없을 때
                    with open(f".\\{SHA256}.exe", 'wb') as f:  # 현 분석 디렉터리에 저장.
                        f.write(EXE_bin)

                    '''
                        CSV에 등록
                    '''
                    if  self.Main_Csv_Manager_instance.Write_to_Csv( # raw새로 APPEND
                            [f".\\{SHA256}.exe", SHA256, str(y_for_predict[Max_index])]
                    ):
                        print("CSV 성공")
                else:
                    '''
                        CSV에 이미 존재하는 SHA256이 있다면, 그 행의 값을 최신화한다.
                    '''
                    print("최신화시도")
                    if self.Main_Csv_Manager_instance.Rewrite_to_Csv( # 이미 존재하는 row를 수정
                            Input_data=[f".\\{SHA256}.exe", SHA256, str(y_for_predict[Max_index])],
                            is_ALL_change=False,
                            specified_hint=SHA256,
                            index_hint=1,
                            is_Append_column_data=False,

                            Rewrite_column=False,
                            columns_list_for_Rewrite_column=None,
                            is_Append_for_Rewrite_column=False
                    ):
                        print("CSV 성공")


        return (y_for_predict, # y 칼럼 이름(list)
                struct.pack('<f', float(list(complete_Prediction)[0][Max_index])), # 예측후 결정된 칼럼의 1.0~0.0 사이 값 (float->bytes)
                str(y_for_predict[Max_index])) # y 칼럼중 판단된 칼럼 이름 (str)

a = AI_Instance(
    Server_VT_api='please INPUT API here',
    VT_Csv_Path = 'VT_analysed.csv',
    Main_Csv_Path='SHARE_ai_exe_dataset.csv',
    Save_Index='idk2'
)

a.Start_Train(
    epoch=100,
    batch_size=2,
    patience=(100//4)
)

SAMPLE_EXE = b''
with open("C:\\wget.exe", 'rb') as f:
    SAMPLE_EXE = f.read()

result = a.Start_Prediction(
    EXE_bin=SAMPLE_EXE,
    Client_VT_api = 'please INPUT API here'
)

print(result)


'''thread_list = []
for i in range(2):
    ace = threading.Thread(target=a.Start_Prediction,args=("please INPUT API here",SAMPLE_EXE))
    ace.start()

while True:
    pass'''
